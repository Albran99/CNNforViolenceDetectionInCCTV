{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lIgnXuz4Jnks","executionInfo":{"status":"ok","timestamp":1706631366680,"user_tz":-60,"elapsed":29924,"user":{"displayName":"GUILLAUME QUINT","userId":"07838362028155105676"}},"outputId":"ddbf2896-74c0-46b3-eccc-e093fac5168a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"markdown","source":["Model definition for all 3D CNNs built from scratch.\n","\n","They require an input_shape for the first layer, which is usually chosen with the dimension of the first sample in the train dataset.\n","\n","There are two versions: the second model is actually simpler than the first one, but the latter is trained on two different versions of the dataset with 2 seconds and 3 seconds bursts."],"metadata":{"id":"m3NH202hXwUo"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"_TRCTATIOJkt"},"outputs":[],"source":["from tqdm.notebook import tqdm\n","import cv2\n","import numpy as np\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv3D, MaxPooling3D, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n","from tensorflow.keras.callbacks import EarlyStopping\n","import os\n","from tensorflow.keras.utils import to_categorical\n","import pickle\n","import sys\n","import random\n","random.seed(33)\n","\n","def 3DFirstModel(input_shape):\n","  model = Sequential()\n","  model.add(Conv3D(16, kernel_size=(1, 3, 3), activation='relu', input_shape=input_shape))\n","  model.add(MaxPooling3D(pool_size=(1, 2, 2)))\n","  model.add(Conv3D(32, kernel_size=(3, 1, 1), activation='relu'))\n","  model.add(MaxPooling3D(pool_size=(2, 1, 1)))\n","  model.add(Flatten())\n","  model.add(Dense(1, activation='sigmoid'))\n","  return model\n","\n","def 3DFSecondModel(input_shape):\n","  model = Sequential()\n","  model.add(Conv3D(16, kernel_size=(3, 3, 3), activation='relu', input_shape=input_shape))\n","  model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n","  model.add(Flatten())\n","  model.add(Dense(1, activation='sigmoid'))\n","  return model\n","\n","def create_cnn(name, input_shape):\n","  if name == '3DFirstModel':\n","    return 3DFirstModel(input_shape)\n","  if name == '3DFSecondModel2Seconds' or name == '3DFSecondModel3Seconds':\n","    return 3DFSecondModel(input_shape)\n","  return None\n"]},{"cell_type":"markdown","source":["Methods used for training and testing the network.\n","\n","For training, provide a model and a dataset, in the format of the load_video_dataset method. The result of the train function is the trained model to be used by the test method. The function also shows a graph of the history training of the model, which contains loss and accuracy for both training and validation.\n","\n","The test function calculates statistics over the testing dataset and plots the corresponding confusion matrix and roc curve"],"metadata":{"id":"B67eYT2wdyeG"}},{"cell_type":"code","source":["def plot_history(history, method):\n","  fig, (ax1, ax2) = plt.subplots(2)\n","  fig.set_size_inches(18.5, 10.5)\n","  # Do not use the default fractional ticks for 'epochs' axis\n","  plt.setp((ax1, ax2), xticks=range(method['EPOCHS']))\n","  # Plot loss\n","  ax1.set_title('Loss')\n","  ax1.plot(history.history['loss'], label = 'train')\n","  ax1.plot(history.history['val_loss'], label = 'test')\n","  ax1.set_ylabel('Loss')\n","  # Determine upper bound of y-axis\n","  max_loss = max(history.history['loss'] + history.history['val_loss'])\n","  ax1.set_ylim([0, np.ceil(max_loss)])\n","  ax1.set_xlabel('Epoch')\n","  ax1.legend(['Train', 'Validation'])\n","  # Plot accuracy\n","  ax2.set_title('Accuracy')\n","  ax2.plot(history.history['accuracy'],  label = 'train')\n","  ax2.plot(history.history['val_accuracy'], label = 'test')\n","  ax2.set_ylabel('Accuracy')\n","  ax2.set_ylim([0, 1])\n","  ax2.set_xlabel('Epoch')\n","  ax2.legend(['Train', 'Validation'])\n","  plt.show()\n","\n","\n","def train(X_train, Y_train, model_name, method):\n","  H5_FILENAME = f'{ROOT_PATH}/Modelli/{model_name}.h5'\n","  try: # load the h5 checkpoint\n","    model = tf.keras.models.load_model(H5_FILENAME)\n","  except: # create and train the model if we don't have a saved h5 checkpoint\n","    print(\"failed to load model. Attempting to recreate it\")\n","    try:\n","      model = create_cnn(model_name, X_train[0].shape)\n","    except:\n","      print('Cannot create CNN')\n","      return None\n","    # Model compilation\n","    model.compile(optimizer=method['OPTIMIZER'], loss='binary_crossentropy', metrics=['accuracy'])\n","    print(\"compile completed\")\n","    # Early stopping over validation loss uses restore_best_weights to save the best model\n","    early_stopping = EarlyStopping(monitor='val_loss', patience=method['PATIENCE'], restore_best_weights=True)\n","    try:\n","      # We save the history of loss and accuracy for both training and validation\n","      history = model.fit(X_train, Y_train, epochs=method['EPOCHS'], batch_size=method['BATCH_SIZE'], validation_split=method['VALIDATION_SPLIT'], callbacks=[early_stopping])\n","    except Exception as e:\n","      print(e)\n","      raise Exception('cannot fit')\n","    # save the h5 checkpoint\n","    model.save(H5_FILENAME)\n","    plot_history(history, method)\n","  print(f\"train completed {model_name}\")\n","  # Return the trained model for it to be tested\n","  return model\n","\n","def test(model, X_test, Y_test):\n","  # Metrics\n","  predictions = model.predict(X_test)\n","  # The threshold is set to 50% for plotting the confusion matrix\n","  # For different error costs scenarios, a different threshold can be chosen\n","  threshold = 0.5\n","  Y_prediction = np.where(predictions > threshold, 1, 0)\n","  # Classification report\n","  # Contains some valuable metrics, like accuracy and recall for the different classes\n","  classification_report = metrics.classification_report(Y_test, Y_prediction, digits = 5)\n","  print(classification_report)\n","  # conf matrix\n","  conf_matrix = confusion_matrix(Y_test, Y_prediction)\n","  disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix,\n","                                display_labels=['Non violence','Violence'])\n","  disp.plot()\n","  # roc\n","  fpr, tpr, th = metrics.roc_curve(Y_test, predictions)\n","  roc_auc = metrics.roc_auc_score(Y_test, predictions)\n","  plt.figure()\n","  plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n","  plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n","  plt.xlim([0.0, 1.0])\n","  plt.ylim([0.0, 1.05])\n","  plt.xlabel('False Positive Rate')\n","  plt.ylabel('True Positive Rate')\n","  plt.title('Receiver operating characteristic')\n","  plt.legend(loc=\"lower right\")\n","  plt.show()\n"],"metadata":{"id":"G3ez9brNdra5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Retrieves the labelled training and testing dataset to be used by the training function.\n","\n","If the specified pickle files are not found in the dataset_path directory, the function fails. To actually build the dataset, use the create_video_dataset function defined in the DatasetPreprocessing file."],"metadata":{"id":"37FeX3Rad1Uq"}},{"cell_type":"code","source":["def load_video_dataset(dataset_path, pkl_config, pickle_name):\n","  train_data = []\n","  train_labels = []\n","  test_data = []\n","  test_labels = []\n","\n","  pickles_dir = f'{ROOT_PATH}/pickles'\n","\n","  train_data_pkl_filepath = f'{pickles_dir}/{pickle_name}-train_data.pkl'\n","  train_label_pkl_filepath = f'{pickles_dir}/{pickle_name}-train_labels.pkl'\n","  test_data_pkl_filepath = f'{pickles_dir}/{pickle_name}-test_data.pkl'\n","  test_label_pkl_filepath = f'{pickles_dir}/{pickle_name}-test_labels.pkl'\n","  train_data, train_labels, test_data, test_labels = None, None, None, None\n","  try:\n","    with open(train_data_pkl_filepath, 'rb') as trd, open(train_label_pkl_filepath, 'rb') as trl, open(test_data_pkl_filepath, 'rb') as ted, open(test_label_pkl_filepath, 'rb') as tel:\n","      train_data = pickle.load(trd)\n","      train_labels = pickle.load(trl)\n","      test_data = pickle.load(ted)\n","      test_labels = pickle.load(tel)\n","    print(\"extracted from cached pickles\")\n","  except FileNotFoundError:\n","    print(f\"Cannot find {pickle_name}. Please create the pickle files using create_video_dataset()\")\n","  except Exception as e:\n","    print(f\"An unexpected error occurred: {e}\")\n","  return train_data, train_labels, test_data, test_labels"],"metadata":{"id":"bUMzt18qduB2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Example usage of the load_video_dataset, train and test function.\n","\n","Due to the dependency between specific dataset versions (with 2 seconds and 3 seconds bursts) and the model used, we redifine `dataset_configs` multiple times with the correct parameters."],"metadata":{"id":"Gg13y5vxd2bK"}},{"cell_type":"code","source":["trainmethod_configs = {\n","  'EPOCHS':  30,\n","  'BATCH_SIZE':  32,\n","  'OPTIMIZER':  'adam',\n","  'VALIDATION_SPLIT':  0.2,\n","  'PATIENCE':  15,\n","  'START_FROM_EPOCH':  5\n","}\n","\n","ROOT_PATH = 'drive/MyDrive/Piras_Quint_Volpi'\n","dataset_path = f'{ROOT_PATH}/Dataset originale'\n"],"metadata":{"id":"SLn1MoO8TliP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset_configs = {\n","  'SIZE':  224,\n","  'FRAMES':  5,\n","  'TRAIN_SPLIT':  0.8,\n","  'FPS':  5,\n","  'CROP':  True\n","}\n","\n","X_train, Y_train, X_test, Y_test = load_video_dataset(dataset_path, dataset_configs, 'default')\n","model = train(X_train, Y_train, '3DFirstModel', trainmethod_configs):\n","results = test(model, X_test, Y_test)"],"metadata":{"id":"EHZnCH9AYb3o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset_configs = {\n","  'SIZE':  224,\n","  'FRAMES':  10,\n","  'TRAIN_SPLIT':  0.8,\n","  'FPS':  5,\n","  'CROP':  True\n","}\n","\n","X_train, Y_train, X_test, Y_test = load_video_dataset(dataset_path, dataset_configs, 'default')\n","model = train(X_train, Y_train, '3DSecondModel2Seconds', trainmethod_configs):\n","results = test(model, X_test, Y_test)"],"metadata":{"id":"ykLa0y4MYoQZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset_configs = {\n","  'SIZE':  224,\n","  'FRAMES':  15,\n","  'TRAIN_SPLIT':  0.8,\n","  'FPS':  5,\n","  'CROP':  True\n","}\n","\n","X_train, Y_train, X_test, Y_test = load_video_dataset(dataset_path, dataset_configs, 'default')\n","model = train(X_train, Y_train, '3DSecondModel3Seconds', trainmethod_configs):\n","results = test(model, X_test, Y_test)"],"metadata":{"id":"apHh5AhUYtRn"},"execution_count":null,"outputs":[]}]}